{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting a Live Endpoint for Real-Time Inference with SageMaker for Customer Churn\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- Image: Data Science\n",
    "- Kernel: Python 3\n",
    "- Instance type: ml.t3.medium\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook builds on previous notebooks where we trained a model to predicts customer churn (i.e., when a company loses a customer).  In this iteration of the notebook, we deploy our trained model to a live endpoint then pass in test data to see how well the model performs predictions.\n",
    "\n",
    "To keep things simple, Experiments have been removed from this version of the notebook.\n",
    "\n",
    "This notebook has been adapted from the [SageMaker examples](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize Environment and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import CSVSerializer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Get the SageMaker session and the execution role from the SageMaker domain\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'live-inference-acg-ai-1221' # Update with the name of a bucket that is already created in S3\n",
    "prefix = 'demo' # The name of the folder that will be created in the S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this lesson, data has already been cleaned and split into two local CSV files: **train.csv** (used to train the model) and **validation.csv** (used to validate how well the model does).\n",
    "\n",
    "We'll take these local files and upload them to S3 so SageMaker can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We trained the model in previous lessons, but to make it easier to follow along with this notebook, we'll do that again here.\n",
    "\n",
    "In this section, we need to specify three things: where our training data is, the path to the algorithm container stored in the Elastic Container Registry, and the algorithm to use (along with hyperparameters).\n",
    "\n",
    "The training job (the Estimator) takes in several hyperparameters.  More information on the hyperparameters for the XGBoost algorithm can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of our training and validation data in S3\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv'\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of the XGBoost container version 1.5-1 (an AWS-managed container)\n",
    "container = sagemaker.image_uris.retrieve('xgboost', sess.boto_region_name, '1.5-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "                    'max_depth':'5',\n",
    "                    'eta':'0.2',\n",
    "                    'gamma':'4',\n",
    "                    'min_child_weight':'6',\n",
    "                    'subsample':'0.8',\n",
    "                    'objective':'binary:logistic',\n",
    "                    'eval_metric':'error',\n",
    "                    'num_round':'100'}\n",
    "\n",
    "# Output path where the trained model will be saved\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# Set up the Estimator, which is training job\n",
    "xgb = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                    hyperparameters=hyperparameters,\n",
    "                                    role=role,\n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge', \n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"fit\" executes the training job\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "\n",
    "Now that we've trained the model, let's deploy it to an endpoint so we can send data to it for live prediction.  We can do that with a single line.  This call to \"deploy\" will create our endpoint configuration and endpoint all at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that our endpoint is live, let's pass in test data to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read test data into a dataframe and transform it into a CSV that can be passed to the endpoint\n",
    "payload = pd.read_csv('test.csv')\n",
    "payload_file = io.StringIO()\n",
    "payload.to_csv(payload_file, header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a low-level client for the SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Client applications use this API to get inferences from the hosted model, by calling invoke_endpoint\n",
    "# We pass in the test data/payload file we read in earlier\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "                            EndpointName=xgb_predictor.endpoint_name, \n",
    "                            Body=payload_file.getvalue(),\n",
    "                            ContentType = 'text/csv'\n",
    ")\n",
    "\n",
    "# Print the response body, which contains the predictions\n",
    "print(response['Body'].read().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
