{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Batch Transform with SageMaker for Customer Churn\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- Image: Data Science\n",
    "- Kernel: Python 3\n",
    "- Instance type: ml.t3.medium\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook builds on previous notebooks where we trained a model to predicts customer churn (i.e., when a company loses a customer).  In this iteration of the notebook, we make use of that trained model to do inference (predictions) on batches of data (loaded from the local file *batch_data.csv*).  To do this, SageMaker will set up and tear down the infrastructure for us, meaning we won't have a persistent, live endpoint at the end like we do with real-time inference.\n",
    "\n",
    "This notebook has been adapted from the [SageMaker examples](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize Environment and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install sagemaker-experiments\n",
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import CSVSerializer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Get the SageMaker session and the execution role from the SageMaker domain\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = '<name-of-your-bucket>' # Update with the name of a bucket that is already created in S3\n",
    "prefix = 'demo' # The name of the folder that will be created in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this lesson, data has already been cleaned and split into two local CSV files: **train.csv** (used to train the model) and **validation.csv** (used to validate how well the model does).\n",
    "\n",
    "We'll take these local files and upload them to S3 so SageMaker can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "In this section, we set up our experiment and trials.  Once they're set up, we can hook into them when we start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "experiment_name = 'batch-transform-churn-experiment'\n",
    "experiment_description = 'A demo experiment'\n",
    "\n",
    "# Use a try-block so we can re-use an existing experiment rather than creating a new one each time\n",
    "try:\n",
    "    experiment = Experiment.create(experiment_name=experiment_name.format(create_date), \n",
    "                                   description=experiment_description)\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} already exists and will be reused.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a trial for the experiment\n",
    "trial_name = \"batch-transform-churn-trial-2\"\n",
    "\n",
    "demo_trial = Trial.create(trial_name = trial_name.format(create_date),\n",
    "                          experiment_name = experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We trained the model in previous lessons, but to make it easier to follow along with this notebook, we'll do that again here.\n",
    "\n",
    "In this section, we need to specify three things: where our training data is, the path to the algorithm container stored in the Elastic Container Registry, and the algorithm to use (along with hyperparameters).\n",
    "\n",
    "The training job (the Estimator) takes in several hyperparameters.  More information on the hyperparameters for the XGBoost algorithm can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of our training and validation data in S3\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv'\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of the XGBoost container version 1.5-1 (an AWS-managed container)\n",
    "container = sagemaker.image_uris.retrieve('xgboost', sess.boto_region_name, '1.5-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up experiment_config, which will be passed to the Estimator; this component will be for the training part only (later on, we'll update the TrialComponentDisplayName for the batch transform job\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': trial_name,\n",
    "                   'TrialComponentDisplayName': 'TrainingJob'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "                    'max_depth':'5',\n",
    "                    'eta':'0.2',\n",
    "                    'gamma':'4',\n",
    "                    'min_child_weight':'6',\n",
    "                    'subsample':'0.8',\n",
    "                    'objective':'binary:logistic',\n",
    "                    'eval_metric':'error',\n",
    "                    'num_round':'100'}\n",
    "\n",
    "# Output path where the trained model will be saved\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# Set up the Estimator, which is training job\n",
    "xgb = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                    hyperparameters=hyperparameters,\n",
    "                                    role=role,\n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge', \n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"fit\" executes the training job\n",
    "# We're passing in experiment_config so that the training results will be tied to the experiment\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, experiment_config=experiment_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host/Batch Transform\n",
    "\n",
    "Now that we've trained the model, let's use it to do prediction on batches of data.  Batch Transform will launch all necessary infrastructure, and then tear it down once the batch transform job completes.\n",
    "\n",
    "For this lesson, we'll be passing in the data from *batch_data.csv*.  IMPORTANT: The dataset used for batch predictions cannot have a target column (i.e., the first column in our case, which represents \"Churn?\".  So we'll remove that column and then upload the local file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data into a dataframe\n",
    "batch_data_path = 'batch_data.csv'\n",
    "df = pd.read_csv(batch_data_path, delimiter=',', index_col=None)\n",
    "\n",
    "batch_data = df.iloc[:, 1:] # delete the target column\n",
    "batch_data.to_csv('batch_data_for_transform.csv', header=False, index = False)\n",
    "\n",
    "# Upload the new CSV file (without the target column) to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'batch/batch_data_for_transform.csv')).upload_file('batch_data_for_transform.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of the batch data used for prediction, and location for batch output\n",
    "s3_batch_input = 's3://{}/{}/batch/batch_data_for_transform.csv'.format(bucket,prefix) \n",
    "s3_batch_output = 's3://{}/{}/batch/batch-inference'.format(bucket, prefix) \n",
    "\n",
    "# Create the Batch Transform job\n",
    "transformer = xgb.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=s3_batch_output\n",
    ")\n",
    "\n",
    "# Update the TrialComponentDisplay name; this is for the transform part of the trial (the previous component was for training)\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': trial_name,\n",
    "                   'TrialComponentDisplayName': 'BatchTransformJob'}\n",
    "\n",
    "transformer.transform(s3_batch_input, content_type=\"text/csv\", split_type=\"Line\", experiment_config = experiment_config)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the batch transform output locally\n",
    "!aws s3 cp --recursive $transformer.output_path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first ten predictions (you can also double-click the file in the folder view to see all predictions)\n",
    "!head batch_data_for_transform.csv.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Experiments\n",
    "\n",
    "In this section, we iterate through our experiments and delete them (this cannot currently be done through the SageMaker UI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to iterate through an experiment to delete its trials, then delete the experiment itself\n",
    "def cleanup_sme_sdk(demo_experiment):\n",
    "    for trial_summary in demo_experiment.list_trials():\n",
    "        trial = Trial.load(trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # Comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # Trial component is associated with another trial\n",
    "                continue\n",
    "            # To prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "        experiment_name = demo_experiment.experiment_name\n",
    "    demo_experiment.delete()\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function above to delete an experiment and its trials\n",
    "# Fill in your experiment name (not the display name)\n",
    "experiment_to_cleanup = Experiment.load(experiment_name='batch-transform-churn-experiment')\n",
    "\n",
    "cleanup_sme_sdk(experiment_to_cleanup)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
